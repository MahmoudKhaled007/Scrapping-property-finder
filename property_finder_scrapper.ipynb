{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Property finder relases all the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scrapping all ad pages  </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>When using Headless option you have to add user agent \n",
    " because Headless mode uses its own default User-Agent if it is not given as an argument.\n",
    " However some webpages may block Headless mode User-Agent to avoid unwanted traffic.\n",
    " It may result in Access denied error while trying to open a webpage. </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total Ads Scrapped:{len(ads_pages)}')\n",
    "\n",
    "# Open a new CSV file for writing\n",
    "with open('Property_finder_ad_pages_final.csv', 'w') as f:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(f)\n",
    "    # Write each link to a new row in the file\n",
    "    writer.writerows([[link] for link in ads_pages])\n",
    "\n",
    "chrome.quit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Options"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapping all properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all Home pages links\n",
    "def scrapp_all_properties():\n",
    "    page_links = [f'https://www.propertyfinder.eg/en/buy/properties-for-sale.html?page={i}' for i in range(1, 4000)]\n",
    "\n",
    "    chrome = Chrome(ChromeDriverManager().install(), options=set_options('--headless','--ignore-certificate-errors','--incognito',\"--disable-gpu\",\"--no-sandbox\",\"--disable-extensions\",\"--dns-prefetch-disable\",\"start-maximized\",\"enable-automation\",\"--disable-dev-shm-usage\",\"--disable-browser-side-navigation\",\"--disable-infobars\",f'user-agent={user_agent}'))\n",
    "    counter=0\n",
    "    ads_pages = []\n",
    "    total_ads_scrapped = 0\n",
    "\n",
    "    # Send a GET request to the webpage\n",
    "    for i in page_links:\n",
    "        try:\n",
    "            chrome.get(i)\n",
    "        except TimeoutException as e:\n",
    "            print(f\"Failed to load page: {i}. Error: {e}\")\n",
    "            continue\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(chrome.page_source, 'html.parser')\n",
    "        # Find and print all links on the page\n",
    "        links = soup.select('a.card__link')\n",
    "        ads_pages.extend(links)\n",
    "        print(f'links visited:{chrome.current_url.split(\"=\")[1]}')\n",
    "        counter=counter+1\n",
    "        if counter==5:\n",
    "            break\n",
    "    print(f'Total Ads Scrapped:{len(ads_pages)}')\n",
    "    #Convert it to set to remove duplicated links\n",
    "    set(ads_pages)\n",
    "    home_page_all_ads=set(ads_pages)\n",
    "    # joblib.dump(ads_pages, 'state2.pkl')\n",
    "    # This line is creating two empty lists, 'href' and 'title', to which we will append values in the loop below.\n",
    "    # This line iterates over each element 'x' in the list 'ads_pages'. \n",
    "    #!For each element x it extracts the 'href' attribute and appends it to the 'href' list, and also extracts the 'title' attribute and appends it to the 'title' list. \n",
    "    #!The results are stored as tuples inside the respective lists.\n",
    "\n",
    "\n",
    "    # Create an empty deque\n",
    "    href_deque = deque()\n",
    "\n",
    "    # Use list comprehension to append items to deque\n",
    "    ([(href_deque.appendleft(x.get('href'))) for x in ads_pages])\n",
    "    joblib.dump(href_deque, 'href_deque_state.pkl')\n",
    "\n",
    "\n",
    "\n",
    "    # Open a new CSV file for writing\n",
    "    # with open('Property_finder_ad_pages.csv', 'wb') as f:\n",
    "    #     # Create a CSV writer object\n",
    "    #     writer = csv.writer(f)\n",
    "    #     # Write each link to a new row in the file\n",
    "    #     writer.writerows([str(link)] for link in ads_pages)\n",
    "    chrome.quit()\n",
    "  \n",
    "scrapp_all_properties()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Save to CSV and check all scrapped</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total Ads Scrapped:{len(ads_pages)}')\n",
    "\n",
    "# Open a new CSV file for writing\n",
    "with open('All_properties_property_finder_ad_pages.csv', 'wb') as f:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(f)\n",
    "    # Write each link to a new row in the file\n",
    "    writer.writerows([str(link)] for link in ads_pages)\n",
    "\n",
    "chrome.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Set options<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from math import ceil\n",
    "from random import sample\n",
    "from time import sleep\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import deque\n",
    "import joblib\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'\n",
    "\n",
    "def set_options(*args):\n",
    "    options = Options()\n",
    "    prefs = {'profile.managed_default_content_settings.images': 2}\n",
    "    options.add_experimental_option('prefs', prefs)\n",
    "    \n",
    "    for arg in args:\n",
    "        options.add_argument(arg)\n",
    "        \n",
    "    #! eager : This strategy causes Selenium to wait for the DOMContentLoaded event (html content downloaded and parsed only).\n",
    "    #!we will use eager because the items we scrapping is loading fast \n",
    "    options.page_load_strategy = 'eager'\n",
    "    return options"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Scrapp Ad Pages </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amigo\\AppData\\Local\\Temp\\ipykernel_38372\\2768155591.py:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  chrome = Chrome(ChromeDriverManager().install(), options=set_options('--headless','--ignore-certificate-errors','--incognito',\"--disable-gpu\",\"--no-sandbox\",\"--disable-extensions\",\"--dns-prefetch-disable\",\"start-maximized\",\"enable-automation\",\"--disable-dev-shm-usage\",\"--disable-browser-side-navigation\",\"--disable-infobars\",f'user-agent={user_agent}'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.propertyfinder.eg/en/plp/buy/chalet-for-sale-north-coast-sidi-abdel-rahman-marassi-3913189.html\n",
      "10 https://www.propertyfinder.eg/en/plp/buy/chalet-for-sale-north-coast-sidi-abdel-rahman-marassi-3913189.html\n",
      "https://www.propertyfinder.eg/en/plp/buy/villa-for-sale-cairo-new-cairo-city-el-katameya-el-katameya-compounds-the-brooks-3856217.html\n",
      "11 https://www.propertyfinder.eg/en/plp/buy/villa-for-sale-cairo-new-cairo-city-el-katameya-el-katameya-compounds-the-brooks-3856217.html\n",
      "https://www.propertyfinder.eg/en/plp/buy/villa-for-sale-cairo-new-cairo-city-the-5th-settlement-5th-settlement-compounds-hyde-park-3732243.html\n",
      "12 https://www.propertyfinder.eg/en/plp/buy/villa-for-sale-cairo-new-cairo-city-the-5th-settlement-5th-settlement-compounds-hyde-park-3732243.html\n",
      "https://www.propertyfinder.eg/en/plp/buy/villa-for-sale-red-sea-hurghada-al-gouna-mangroovy-residence-3725705.html\n",
      "13 https://www.propertyfinder.eg/en/plp/buy/villa-for-sale-red-sea-hurghada-al-gouna-mangroovy-residence-3725705.html\n",
      "https://www.propertyfinder.eg/en/plp/buy/apartment-for-sale-cairo-new-capital-city-new-capital-compounds-il-bosco-3868845.html\n",
      "14 https://www.propertyfinder.eg/en/plp/buy/apartment-for-sale-cairo-new-capital-city-new-capital-compounds-il-bosco-3868845.html\n",
      "https://www.propertyfinder.eg/en/plp/buy/chalet-for-sale-north-coast-sidi-abdel-rahman-marassi-3731149.html\n",
      "15 https://www.propertyfinder.eg/en/plp/buy/chalet-for-sale-north-coast-sidi-abdel-rahman-marassi-3731149.html\n",
      "https://www.propertyfinder.eg/en/plp/buy/apartment-for-sale-giza-6-october-city-6-october-compounds-october-plaza-3897252.html\n",
      "16 https://www.propertyfinder.eg/en/plp/buy/apartment-for-sale-giza-6-october-city-6-october-compounds-october-plaza-3897252.html\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the amenities\n",
    "#DONE add joblib Done\n",
    "#DONE add deqeue Done\n",
    "#DONE save all variables into dictionary to dump it to pickle with joblib  Done\n",
    "\n",
    "amenities_list = []\n",
    "# ads_list_dic=[]\n",
    "\n",
    "\n",
    "chrome = Chrome(ChromeDriverManager().install(), options=set_options('--headless','--ignore-certificate-errors','--incognito',\"--disable-gpu\",\"--no-sandbox\",\"--disable-extensions\",\"--dns-prefetch-disable\",\"start-maximized\",\"enable-automation\",\"--disable-dev-shm-usage\",\"--disable-browser-side-navigation\",\"--disable-infobars\",f'user-agent={user_agent}'))\n",
    "try:    \n",
    "    href_deque = joblib.load('href_deque_state.pkl')\n",
    "    ads_list_dic=joblib.load('ads_scrapped.pkl')\n",
    "except:\n",
    "    #FIXME not logic to scrapp all again \n",
    "    # scrapp_all_properties()\n",
    "    href_deque = joblib.load('href_deque_state.pkl')\n",
    "\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    while href_deque:\n",
    "        #it will pop from the left (newest added) to the oldest \n",
    "        # href_deque.popleft()\n",
    "        current_url = href_deque.popleft()\n",
    "        ad={'url':current_url}\n",
    "\n",
    "        print(current_url)\n",
    "        scraped_urls = {x['url'] for x in ads_list_dic}\n",
    "        \n",
    "        if current_url not in scraped_urls:\n",
    "            print(len(ads_list_dic), current_url)\n",
    "            while True:\n",
    "                    try:\n",
    "                        chrome.get(current_url)\n",
    "                        #sleep(2.0)\n",
    "                        break\n",
    "                    except TimeoutException:\n",
    "                        print('timeout detected... cooling off...')\n",
    "                        sleep(2.0)\n",
    "                        # chrome.get(href_deque[0])\n",
    "                        #TODO make this dynamic \n",
    "                        \n",
    "            soup = BeautifulSoup(chrome.page_source, 'html.parser')\n",
    "            # Find and print subtitle of the page\n",
    "            #!####################################################\n",
    "            ad['title'] = soup.find('h1',class_ = 'property-page__title').text.strip()\n",
    "            ad['sub_title'] = soup.find('h2',class_ = 'property-page__sub-title').text.strip()\n",
    "            li = soup.find_all('div',class_ = 'property-facts__value')\n",
    "            ad['property_type']=soup.find_all('div',class_ = 'property-facts__value')[0].text.strip()\n",
    "            #Get all text\n",
    "            property_size_temp=soup.find_all('div',class_ = 'property-facts__value')[1].text.strip()\n",
    "            #Extract SQM size\n",
    "            ad['property_size']=re.sub(r'\\D*(\\d+)\\D*',r'\\1', li[1].text.strip()[15:]).strip()\n",
    "            ad['bedrooms']=soup.find_all('div',class_ = 'property-facts__value')[2].text.replace('\\n','').replace('\\t', '').strip()\n",
    "            ad['bathrooms']=soup.find_all('div',class_ = 'property-facts__value')[3].text.strip()\n",
    "            ad['agent_name']=soup.find('h4',class_ = 'property-agent__name').text.strip()\n",
    "            ad['agent_company']=soup.find('div',class_ = 'property-agent__position-broker-name').text.strip()\n",
    "            ad['agent_number'] = soup.find('a', class_='property-contact__primary-button').get('href')[4:]\n",
    "            ad['description']=soup.find('div',class_ = 'property-description__text-trim').text.replace('\\n','').strip()\n",
    "            ad['listed_date']=soup.find_all('div',class_ = 'property-page__legal-list-content')[1].text.strip()\n",
    "            #! Ameneties \n",
    "            # Get all the divs with class 'property-amenities__list' from a BeautifulSoup object called soup\n",
    "            amenities_all_divs = soup.find_all('div', class_='property-amenities__list')\n",
    "            # Iterate over each div in the amenities_all_divs list and add its text content (with leading/trailing whitespace removed) to the amenities_list\n",
    "            [amenities_list.append(x.text.strip()) for x in amenities_all_divs]\n",
    "            # Join the elements of the amenities_list together into a single string, separated by semicolons (;), and assign the result to the amenities variable\n",
    "            ad['amenities'] = \";\".join(amenities_list)\n",
    "\n",
    "            #Add Dictionary to list \n",
    "            ads_list_dic.append(ad)\n",
    "            joblib.dump(ads_list_dic, 'ads_scrapped.pkl')\n",
    "            joblib.dump(href_deque, 'href_deque_state.pkl')\n",
    "\n",
    "            #pop first left link \n",
    "chrome.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
